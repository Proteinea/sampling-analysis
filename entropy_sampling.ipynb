{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3 threads\n",
      "Read 500 sequences (type: Protein) from msa_input.fasta\n",
      "Calculating pairwise ktuple-distances...\n",
      "Ktuple-distance calculation progress done. CPU time: 14.00u 0.00s 00:00:14.00 Elapsed: 00:00:05\n",
      "Guide-tree computation done.\n",
      "Progressive alignment progress done. CPU time: 13.26u 0.08s 00:00:13.34 Elapsed: 00:00:07\n",
      "Alignment written to _temp_msa_output.fasta\n"
     ]
    }
   ],
   "source": [
    "from stateval.src.configs import MSAConfig\n",
    "from stateval.src.msa import Msa \n",
    "from stateval.src.sh_entropies import ShannonEntropies \n",
    "from stat_eval_utils import fasta2dict\n",
    "\n",
    "\n",
    "sequences = fasta2dict(\"./mdh_train_sample.fasta\")\n",
    "sequences = {id: seq for id, seq in zip(sequences[\"ID\"], sequences[\"Seq\"])}\n",
    "\n",
    "msa = Msa(MSAConfig(max_gap_ratio=0.8))\n",
    "aligned_seqs = msa.align(sequences)\n",
    "se = ShannonEntropies()\n",
    "entropies = se.calculate_entropies(aligned_seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(v) for v in sequences.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import yaml\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "with open('generation_config.yml', 'r') as file:\n",
    "    config  = yaml.safe_load(file)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model_checkpoint_path\"])\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add a padding token\n",
    "model = AutoModelForCausalLM.from_pretrained(config[\"model_checkpoint_path\"], device_map=\"auto\", load_in_8bit=True)\n",
    "model.to_bettertransformer()\n",
    "model.config.max_length = 512  #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, max_new_tokens, segments_indicies, temps, prompt, top_k=None, eos_token_id=0):\n",
    "    temp_index = 0\n",
    "    idx = prompt\n",
    "    past_key_values = None\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        outputs = model(idx, past_key_values=past_key_values)\n",
    "        logits = outputs.logits\n",
    "        past_key_values = outputs.past_key_values\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temps[temp_index]\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # either sample from the distribution or take the most likely element\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        if idx.shape[1] > segments_indicies[temp_index] and temp_index +1 < len(temps):\n",
    "            temp_index += 1\n",
    "        \n",
    "        if idx_next.item() == eos_token_id:\n",
    "            break\n",
    "    return idx.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8504, 0.8922, 0.8676, 0.9131, 0.9812, 1.0258, 1.1472]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_segments = 7\n",
    "indicies = np.linspace(0, len(entropies), n_segments+1).astype(int)\n",
    "entropies_median = [np.median(entropies[indicies[i]: indicies[i+1]]) for i in range(n_segments)]\n",
    "temps = [v* 2 for v in entropies_median]\n",
    "temps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "device = \"cuda\"\n",
    "generated_sequences = []\n",
    "batch_size = 1\n",
    "seqs = fasta2dict(\"./mdh_train_sample.fasta\")[\"Seq\"]\n",
    "for batch_idx in tqdm(range(0, len(sequences), batch_size)):\n",
    "        inputs = tokenizer.batch_encode_plus(\n",
    "            seqs[batch_idx:batch_idx+batch_size],\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=512,  # Set the maximum length to 512\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        input_ids = inputs.input_ids\n",
    "        attention_mask = inputs.attention_mask\n",
    "        prompt = input_ids[:, :config[\"sequence_prompt_index\"]]\n",
    "        # Generate multiple prompt\n",
    "        with torch.no_grad():\n",
    "\n",
    "                outputs = generate(model, input_ids.shape[1], indicies[1:], temps, prompt.clone().long())\n",
    "                # outputs = model.generate(\n",
    "                    \n",
    "                #     attention_mask=attention_mask[:, :config[\"sequence_prompt_index\"]],\n",
    "                #     **config[\"generate_kwargs\"]\n",
    "                #     # num_return_sequences=5\n",
    "                # )\n",
    "            # Decode and store the generated sequences\n",
    "        for generated_output in outputs:\n",
    "            generated_text = tokenizer.decode(generated_output, skip_special_tokens=True)\n",
    "            generated_text = generated_text.replace(\"|endoftext|>\", \"\")  # Remove the header\n",
    "            generated_sequences.append(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
